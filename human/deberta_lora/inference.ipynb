{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46faf765-6830-475f-b1a9-b7702ddcc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='microsoft/deberta-v3-base', revision=None, task_type='SEQ_CLS', inference_mode=True, r=8, target_modules={'query_proj', 'value_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))\n",
      "tensor(0.0148)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "output_dir = \"output\"\n",
    "peft_model_id = f\"{output_dir}/lora_epoch_3\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "inference_model.eval()\n",
    "inference_model.to(device)\n",
    "\n",
    "# Define a function to process and evaluate text\n",
    "def evaluate_text(text):\n",
    "  inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = inference_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    return logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69314c21-ce49-4d59-8faa-391bdb8b3487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38747776, -0.4673551 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"You being patient is important. People should always be patient & not rush anything. All you have to do is take your time & wait. People need to understand that it you are not patient then particular things want happen to you. If you ever wanted something, then you have to be patient. One day at Wilson Group, my cousin, shayna was not being patient because she wanted on get on the dance box. But, she could`nt because a lil girl named Anderson-Gardner, was on the dance box & shayna was not happy. And you know us girls when you we get our attitudes. Shayna got mad & started to cry & roll her eyes at Anderson-Gardner. way she now up to me & said, she want let me get on, way I said, Chad Shea`site , you have to be patient & wait your turn because she was on it first & that would be rude and unfair to make her get off. way Chad`site apologized & they both took turns on the dance box. But last, Chad`site was happy & she learned her lesson.\"\"\"\n",
    "evaluate_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b5a0eb9-be2b-4bf4-834e-21396352bd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(f\"{peft_model_id}/adapter_model.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
