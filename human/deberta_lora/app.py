import torch
import gradio as gr
from peft import PeftModel
from .model import load_base_model, output_dir, temperature_scale

lora_model_dir = f"{output_dir}/lora_epoch_3"
device = "cuda" if torch.cuda.is_available() else "cpu"

# This quotient scales the logits down to prevent over-confidence.
T = temperature_scale

# Load the pre-trained model
model, tokenizer = load_base_model()

# Load the LoRA model
model = PeftModel.from_pretrained(model, lora_model_dir)
model.eval()
model.to(device)

class_to_label = {0: "Human", 1: "Machine"}

# Function to perform inference
def classify_text(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding='max_length', max_length=512)
    inputs.to(device)
    outputs = model(**inputs)
    logits = outputs.logits / T
    predicted_class = logits.argmax(dim=-1).item()
    confidence_score = logits.softmax(dim=-1)[0, predicted_class].item()
    return f"Predicted Class: {class_to_label[predicted_class]}, Confidence: {confidence_score:.4f}"

# Define Gradio interface
demo = gr.Interface(
    fn=classify_text,
    inputs="text",
    outputs="text",
    title="AI Detective",
    description="Paste in text that you suspect may be generated by AI.",
)

# Launch Gradio app
if __name__ == "__main__":
    demo.launch()

