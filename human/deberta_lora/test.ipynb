{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9b4ed67-14a3-44a4-844d-a766735e5375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchook/.local/opt/miniconda3/envs/gan-ai/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"../data\"\n",
    "output_dir = \"./output\"\n",
    "peft_model_id = f\"{output_dir}/lora_epoch_3\"\n",
    "\n",
    "def clean_text_data(input: str):\n",
    "  \"\"\"\n",
    "  Clean text data by removing special characters and extra spaces\n",
    "  \"\"\"\n",
    "  return ' '.join(input.split())\n",
    "\n",
    "\n",
    "def load_and_prepare_dataset(data_file, tokenizer, batch_size):\n",
    "  \"\"\"\n",
    "  Helper function to load and prepare a CSV dataset for training or testing\n",
    "  \"\"\"\n",
    "  def tokenize_fn(examples):\n",
    "    examples['essay'] = [clean_text_data(essay) for essay in examples['essay']]\n",
    "    return tokenizer(examples['essay'], truncation=True, padding='max_length', max_length=512)\n",
    "  dataset = load_dataset('csv', data_files={'train': data_file})\n",
    "  # dataset['train'] = dataset['train'].select(range(2000))\n",
    "  tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "  tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "  tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "  return DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(peft_config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a85fd-17ce-4ece-8035-287a4e44a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f5e9616-0364-47a2-b50d-21456b6c1be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0414, 0.3616]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"test\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "523c2c78-3165-4835-b988-8f38fe5a1759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   1%|▌                                                                                                   | 4/649 [00:00<00:33, 19.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1]\n",
      "[1 0 1 0]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4054, Smoothed Acc: 0.0000:  16%|██████████▉                                                         | 104/649 [00:05<00:27, 19.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n",
      "[1 0 0 0]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1435, Smoothed Acc: 0.0025:  31%|█████████████████████▎                                              | 204/649 [00:10<00:22, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1]\n",
      "[1 0 1 0]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2575, Smoothed Acc: 0.0000:  47%|███████████████████████████████▊                                    | 304/649 [00:15<00:17, 19.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0]\n",
      "[1 0 1 1]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2026, Smoothed Acc: 0.0000:  62%|██████████████████████████████████████████▎                         | 404/649 [00:20<00:12, 19.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n",
      "[1 0 0 1]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1433, Smoothed Acc: 0.0025:  78%|████████████████████████████████████████████████████▊               | 504/649 [00:25<00:07, 19.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0]\n",
      "[1 0 1 1]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1544, Smoothed Acc: 0.0000:  93%|███████████████████████████████████████████████████████████████▎    | 604/649 [00:31<00:02, 19.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0]\n",
      "[0 0 1 1]\n",
      "tensor([False, False, False, False], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1638, Smoothed Acc: 0.0000: 100%|████████████████████████████████████████████████████████████████████| 649/649 [00:33<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0008\n",
      "Precision: 0.0008\n",
      "Recall: 0.0008\n",
      "F1-Score: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and tokenize\n",
    "eval_dataloader = load_and_prepare_dataset(f'{data_dir}/test.csv', tokenizer, batch_size=4)\n",
    "\n",
    "# Initialize metrics storage\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluation loop\n",
    "progress_bar = tqdm(eval_dataloader, desc=\"Eval\")\n",
    "smoothed_accuracy = []\n",
    "\n",
    "try:\n",
    "  with torch.no_grad():\n",
    "    for idx, batch in enumerate(progress_bar):\n",
    "      labels = batch['labels']\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(**batch)\n",
    "      logits = outputs.logits\n",
    "\n",
    "      # Predictions and labels\n",
    "      predictions = torch.argmax(logits, dim=-1)\n",
    "      all_predictions.extend(predictions.cpu().numpy())\n",
    "      all_labels.extend(labels.cpu().numpy())\n",
    "      \n",
    "      accuracy = (predictions == batch[\"labels\"]).float().mean().item()\n",
    "      if (idx % 100 == 0):\n",
    "        print(predictions.cpu().numpy())\n",
    "        print(labels.cpu().numpy())\n",
    "        print(predictions == batch[\"labels\"])\n",
    "      smoothed_accuracy.append(accuracy)\n",
    "\n",
    "      if len(smoothed_accuracy) > 100:\n",
    "        smoothed_accuracy.pop(0)\n",
    "        smooth_acc = np.mean(smoothed_accuracy)\n",
    "        progress_bar.set_description(f\"Loss: {outputs.loss.item():.4f}, Smoothed Acc: {smooth_acc:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Error occurred during evaluation: {e}\")\n",
    "  torch.cuda.empty_cache()  # Free up VRAM if an error occurs\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8517a-3c85-48e8-aa4e-ad093729dfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
